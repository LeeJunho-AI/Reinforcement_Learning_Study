{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **Introduction**\n",
        "\n",
        "### **Importance Sampling**\n",
        "\n",
        "Importance sampling refers to a method for estimating the expected value of a distribution \\( A \\), using samples drawn from another distribution \\( B \\).\n",
        "\n",
        "In this approach, a **weighting factor**, known as the **importance sampling ratio**, is applied to the sampled values to adjust for the difference between the two distributions. The variance of this method depends on how similar the two distributions are.\n",
        "\n",
        "---\n",
        "\n",
        "#### Example\n",
        "\n",
        "Suppose the distribution \\( A \\) always selects the action **\"UP\"** with 100% probability, while the distribution \\( B \\) selects **\"UP\"** with only 1% probability. In this case, the importance sampling ratio becomes:\n",
        "\n",
        "$$\n",
        "\\text{Ratio} = \\frac{P_A(\\text{UP})}{P_B(\\text{UP})} = \\frac{1}{0.01} = 100\n",
        "$$\n",
        "\n",
        "Thus, the sampled value is scaled by 100, leading to a significant increase in its weight.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Insight\n",
        "\n",
        "When the two distributions \\( A \\) and \\( B \\) are similar, the probabilities for actions will be closer, resulting in a **lower ratio**. A smaller ratio reduces the variance of the estimate, making it more stable and efficient. Conversely, a large discrepancy between the distributions leads to higher variance, which can degrade the quality of the estimation.\n",
        "\n",
        "---\n",
        "\n",
        "ðŸ’¡ **Takeaway**  \n",
        "The closer the probability distributions \\( A \\) and \\( B \\) are, the better the performance of importance sampling due to reduced variance in the weighted estimates.\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "As the distributions diverge, the variance increases, leading to greater errors in the estimation of the expected value. This phenomenon can be observed from the following results:"
      ],
      "metadata": {
        "id": "DiQZTok7zpbP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "NVyHYOmgwZL_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def simulate_dice(target_policy, behavior_policy, num_epochs):\n",
        "  weighted_face = []\n",
        "  for episode in range(total_episode):\n",
        "    face = np.random.choice(dice_faces, p = behavior_policy)\n",
        "    idx = np.where(dice_faces == face)\n",
        "    rho = target_policy[idx] / behavior_policy[idx]\n",
        "    weighted_face.append(face * rho)\n",
        "  mean = np.mean(weighted_face)\n",
        "  var = np.var(weighted_face)\n",
        "  return mean, var"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Define dice faces (1 to 6)\n",
        "dice_faces = np.array([1, 2, 3, 4, 5, 6])\n",
        "\n",
        "# Define the behavior policy: biased dice with 70% probability for face 6\n",
        "behavior_policy = np.array([0.3/5, 0.3/5, 0.3/5, 0.3/5, 0.3/5, 0.7])\n",
        "\n",
        "# Define the target policy: biased dice with 50% probability for face 1\n",
        "target_policy = np.array([0.5, 0.1, 0.1, 0.1, 0.1, 0.1])\n",
        "\n",
        "# Number of episodes to simulate\n",
        "total_episode = 1000\n",
        "\n",
        "mean, var = simulate_dice(target_policy, behavior_policy, total_episode)\n",
        "true_expected_value = np.sum(dice_faces * target_policy)\n",
        "\n",
        "print(f\"ðŸŽ² Estimated Expected Value: {mean:.4f}\")\n",
        "print(f\"ðŸŽ²True Expcted Value: {true_expected_value}\")\n",
        "print(f\"ðŸ“Š Variance: {var:.4f}\")\n",
        "print(f\"ðŸ“Š Error: {np.abs((mean-true_expected_value)/true_expected_value) * 100:.4f} %\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY6C4HejzEBb",
        "outputId": "2557df86-87b0-4e6d-df4c-722604c0326c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ² Estimated Expected Value: 2.6046\n",
            "ðŸŽ²True Expcted Value: 2.5\n",
            "ðŸ“Š Variance: 7.7602\n",
            "ðŸ“Š Error: 4.1848 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dice_faces = np.array([1, 2, 3, 4, 5, 6])\n",
        "behavior_policy = np.array([0.6, 0.4/5, 0.4/5, 0.4/5, 0.4/5, 0.4/5])\n",
        "target_policy = np.array([0.5, 0.1, 0.1, 0.1, 0.1, 0.1])\n",
        "\n",
        "total_episode = 1000\n",
        "\n",
        "mean, var = simulate_dice(target_policy, behavior_policy, total_episode)\n",
        "true_expected_value = np.sum(dice_faces * target_policy)\n",
        "\n",
        "print(f\"ðŸŽ² Estimated Expected Value: {mean:.4f}\")\n",
        "print(f\"ðŸŽ²True Expcted Value: {true_expected_value}\")\n",
        "print(f\"ðŸ“Š Variance: {var:.4f}\")\n",
        "print(f\"ðŸ“Š Error: {np.abs((mean-true_expected_value)/true_expected_value) * 100:.4f} %\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvGpNvgrzLtA",
        "outputId": "3c868c49-70ec-4859-b6d7-5c8c349e0aa0"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ² Estimated Expected Value: 2.4471\n",
            "ðŸŽ²True Expcted Value: 2.5\n",
            "ðŸ“Š Variance: 5.1474\n",
            "ðŸ“Š Error: 2.1167 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dice_faces = np.array([1, 2, 3, 4, 5, 6])\n",
        "behavior_policy = np.array([0, 0.2, 0.2, 0.2, 0.2, 0.2])\n",
        "target_policy = np.array([1, 0, 0, 0, 0, 0])\n",
        "\n",
        "total_episode = 100\n",
        "\n",
        "mean, var = simulate_dice(target_policy, behavior_policy, total_episode)\n",
        "\n",
        "print(f\"ðŸŽ² Estimated Expected Value: {mean:.4f}\")\n",
        "print(f\"ðŸŽ²True Expcted Value: 3.5\")\n",
        "print(f\"ðŸ“Š Variance: {var:.4f}\")\n",
        "print(f\"ðŸ“Š Error: {np.abs((mean-true_expected_value)/true_expected_value) * 100:.4f} %\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN5fpymqzZHE",
        "outputId": "13d56e01-8ad1-43eb-88ac-334a0229ddc9"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ² Estimated Expected Value: 0.0000\n",
            "ðŸŽ²True Expcted Value: 3.5\n",
            "ðŸ“Š Variance: 0.0000\n",
            "ðŸ“Š Error: 100.0000 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **Conclusion**\n",
        "\n",
        "From the results above, we confirm that when the two probability distributions are more similar, the **variance decreases**, and the **error in the estimated expected value** reduces.\n",
        "\n",
        "However, in the last case, if actions generated by the behavior policy are not sampled in the target policy, the **importance sampling ratio** will always be 0, and the estimated expected value will consequently be 0.\n",
        "\n",
        "To prevent this, the behavior policy must assign a non-zero probability to any action that has a non-zero probability in the target policy. This requirement is called the **coverage assumption**.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "JiDWeF5Z3vHQ"
      }
    }
  ]
}